(venv) (base) C:\Users\Alice\NLP\alice\nlp\project\sarcasm\sarcasm-detector\svm>python utils.py
====December 10, 2018 07:57:23 PM====
creating new dict
num sarcastic tweets: 25273
num non sarcastic tweets: 117825
creating 15000 most common sarcastic and non-sarcastic ngram sets...
getting freq sarcastic and non-sarcatistic ngram counts...
Scoring sentiment in 40000 tweets ...
..................................................
..................................................
..................................................
..................................................
..................................................
..................................................
..................................................
..................................................
[(0.0, 5724), (2.0, 2848), (1.0, 2367), (3.0, 2075), (-2.0, 1955), (-1.0, 1844), (4.0, 1822), (-3.0, 1682), (5.0, 1636), (-4.0, 1221), (6.0, 1143), (7.0, 989), (-5.0, 922), (8.0, 863), (9.0, 762), (11.0, 762), (10.0, 704), (-6.0, 670), (13.0, 568), (12.0, 543)]
Tweets with scored words: 36999; total words scored: 127153
Total word/tags with scores: 6588
getting freq sarcastic and non-sarcatistic ngram counts...
Scoring sentiment in 23098 tweets ...
..................................................
..................................................
..................................................
..................................................
..............................[(0.0, 3412), (2.0, 1735), (1.0, 1443), (-2.0, 1284), (3.0, 1184), (-1.0, 1128), (4.0, 1064), (-3.0, 990), (5.0, 905), (-4.0, 753), (6.0, 680), (-5.0, 590), (7.0, 558), (8.0, 452), (-6.0, 445), (11.0, 395), (9.0, 368), (10.0, 352), (-7.0, 335), (-8.0, 307)]
Tweets with scored words: 21341; total words scored: 73110
Total word/tags with scores: 7754
====December 10, 2018 08:02:58 PM====

====December 10, 2018 08:02:58 PM====
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ MaxEnt ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

training max_ent with penalty=l2, solver=sag, C=0.1, and class_weight=balanced
test [1 1 1 1 1 1 1 1 1 1] [0 0 0 0 0 0 0 0 0 0]
pred [0 1 1 0 1 0 1 0 1 0] [0 0 0 0 0 1 0 0 0 0]
mean squared error: 0.19759286518313274
pearson coefficient: (0.4590540268432643, 0.0)
f-score: 0.5877145438121046

====December 10, 2018 08:02:59 PM====
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ SVM ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

training svm with kernel=rbf, C=0.01, gamma=scale, class_weight=balanced
test [1 1 1 1 1 1 1 1 1 1] [0 0 0 0 0 0 0 0 0 0]
pred [0 1 1 0 0 0 1 0 1 0] [0 0 0 0 0 0 0 0 0 0]
mean squared error: 0.19642393280803533
pearson coefficient: (0.440999675338861, 0.0)
f-score: 0.5681104236078057
====December 10, 2018 08:03:42 PM====

(venv) (base) C:\Users\Alice\NLP\alice\nlp\project\sarcasm\sarcasm-detector\svm>
